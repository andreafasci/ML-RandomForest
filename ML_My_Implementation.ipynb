{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import useful stuff from libraries etc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class MyRandomForest, inheriting BaseEstimator class (BaseEstimator is used only for CrossValidation purposes)\n",
    "class MyRandomForest(BaseEstimator):\n",
    "    \n",
    "    # Class constructor, default values are here selected, but other values can be specified when creating the RF)\n",
    "    def __init__(self, n_estimators=100, random_state=None, max_depth=None,\n",
    "                 max_features=None, min_samples_leaf=1, min_samples_split=2,\n",
    "                class_weight=None, criterion='gini'):\n",
    "        # Create an array of Decision Trees\n",
    "        self.__listOfDecisionTrees = []\n",
    "        # For each Decision Tree, save which are the features used by this particular Tree\n",
    "        self.__listOfFeaturesPerTree = []\n",
    "        # Save the number of estimators\n",
    "        self.__n_estimators = n_estimators\n",
    "        \n",
    "        # Save other parameters for trees\n",
    "        self.__random_state = random_state\n",
    "        self.__max_depth =  max_depth\n",
    "        self.__max_features = max_features\n",
    "        self.__min_samples_leaf = min_samples_leaf\n",
    "        self.__min_samples_split = min_samples_split  \n",
    "        self.__class_weight = class_weight\n",
    "        self.__criterion = criterion\n",
    "        \n",
    "        # Populate the list of decision trees\n",
    "        for i in range(0,n_estimators):\n",
    "            clf = DecisionTreeClassifier(\n",
    "                criterion = 'gini',\n",
    "                random_state = self.__random_state,\n",
    "                max_depth = self.__max_depth,\n",
    "                max_features = self.__max_features,\n",
    "                min_samples_leaf = self.__min_samples_leaf,\n",
    "                min_samples_split = self.__min_samples_split\n",
    "                )\n",
    "            self.__listOfDecisionTrees.append(clf)\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        # Fit the trees on X,Y data, sampling randomly data and features for each tree        \n",
    "        for i in range(0, self.__n_estimators):\n",
    "            # Add to data (X) the associated labels in Y \n",
    "            X['labels_Y'] = Y\n",
    "            #Sample, with replacement, training examples from X, Y\n",
    "            X_samples_1 = X.sample(frac=1.0, replace=True)\n",
    "            # Sample features (taking the transpose, I'll sample and retranspose again the result)\n",
    "            # Since we're not considering the last column, there's no risk to sample also the label as a feature\n",
    "            X_samples_2 = X_samples_1.T[0:len(X_samples_1.columns)-1].sample(frac=1.0, replace=False).T\n",
    "            # Reassociate the labels\n",
    "            X_samples_2['labels_Y'] = X_samples_1['labels_Y']\n",
    "            \n",
    "            # Create an array to save which features will be used (excluding 'labels_Y')\n",
    "            features = X_samples_2.columns[:len(X_samples_2.columns)-1]\n",
    "        \n",
    "            # Save the list of features considered in this tree, will be used when predicting\n",
    "            self.__listOfFeaturesPerTree.append(features.tolist())\n",
    "            # Fit the decision tree on the selected samples and features\n",
    "            self.__listOfDecisionTrees[i].fit(X_samples_2[features], X_samples_2['labels_Y'])\n",
    "\n",
    "        return self    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Predict labels of new input points X\n",
    "        \n",
    "        # Array in which there will be labels suggested by each tree, for each \"row\" of the data.\n",
    "        # For each row, the majority of the votes will then be taken\n",
    "        votes = []\n",
    "        \n",
    "        # For each tree, predict the labels of each row and save the results\n",
    "        for i in range(0, self.__n_estimators):\n",
    "            features = self.__listOfFeaturesPerTree[i]\n",
    "            votes.append(self.__listOfDecisionTrees[i].predict(X[features]))\n",
    "        \n",
    "        # For each row, compute which was the most present label and select it as final result\n",
    "        final_votes = []\n",
    "        for i in range (0, len(X)):\n",
    "            counts = np.bincount([votes[j][i] for j in range(0,10)])\n",
    "            final_votes.append(np.argmax(counts))\n",
    "            \n",
    "        return final_votes\n",
    "    \n",
    "    def score(self, X, Y):\n",
    "        # Given unlabeled data X and corresponding true labels Y,\n",
    "        #  return a value in [0,1] representing percentage of correctly labeled data\n",
    "        predictions = self.predict(X)\n",
    "        matching = [predictions == Y.values]\n",
    "        totalMatches = np.sum(matching) / len(predictions)\n",
    "        return totalMatches\n",
    "    \n",
    "    # Function used for cross validation, get actual parameters of the Random Forest\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'n_estimators' : self.__n_estimators,\n",
    "            'random_state' : self.__random_state,\n",
    "            'max_depth' : self.__max_depth,\n",
    "            'max_features' : self.__max_features,\n",
    "            'min_samples_leaf' : self.__min_samples_leaf,\n",
    "            'min_samples_split' : self.__min_samples_split,  \n",
    "            'class_weight' : self.__class_weight,\n",
    "            'criterion' : self.__criterion\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from file into a Pandas dataframe\n",
    "df = pd.read_csv('processed.cleveland.data', header=None)\n",
    "\n",
    "# Assign names (and types) of the column to the corresponding data\n",
    "df.columns = ['age','sex','cp', 'trestbps','chol','fbs', 'restecg', 'thalach','exang','oldpeak','slope','ca','thal','num']\n",
    "df = df.astype({'age': int, 'sex': int, 'cp': int, 'trestbps': int, 'chol': int, 'fbs': int, 'restecg': int, 'thalach':int, 'exang': int, 'slope': int, 'ca':int, 'thal':int})\n",
    "\n",
    "# Apply one-hot-encoding: for each categorical feature, add the corresponding columns\n",
    "df = pd.concat([df, pd.get_dummies(df['cp'], prefix='cp')], axis=1)\n",
    "df = pd.concat([df, pd.get_dummies(df['restecg'], prefix='restecg')], axis=1)\n",
    "df = pd.concat([df, pd.get_dummies(df['slope'], prefix='slope')], axis=1)\n",
    "df = pd.concat([df, pd.get_dummies(df['thal'], prefix='thal')], axis=1)\n",
    "\n",
    "# Remove the \"old\" columns\n",
    "df.drop(['cp'],axis=1, inplace=True)\n",
    "df.drop(['restecg'],axis=1, inplace=True)\n",
    "df.drop(['slope'],axis=1, inplace=True)\n",
    "df.drop(['thal'],axis=1, inplace=True)\n",
    "\n",
    "# Create a new column called presence representing the presence of heart disease \n",
    "df['presence'] = df['num'] > 0 \n",
    "# If we want to consider the problem as Multiclass classification, comment the previous row and uncomment \n",
    "#  the next one\n",
    "#df['presence'] = df['num']\n",
    "\n",
    "# Select the type of data in the new column\n",
    "df = df.astype({'presence': int})\n",
    "\n",
    "# Rearrange columns order\n",
    "df = df[['age', 'sex', 'trestbps', 'chol', 'fbs', 'thalach', 'exang', 'oldpeak',\n",
    "       'ca', 'cp_1', 'cp_2', 'cp_3', 'cp_4', 'restecg_0', 'restecg_1',\n",
    "       'restecg_2', 'slope_1', 'slope_2', 'slope_3', 'thal_3', 'thal_6',\n",
    "       'thal_7', 'num', 'presence']]\n",
    "\n",
    "# Function that, given a length of an array and a percentage p, \n",
    "# it will return p% of indexes of that array, randomly chosen, sorted\n",
    "def defineTraining (length, percentage=0.8):\n",
    "    sample = random.sample(range(0, length), int(percentage*length))\n",
    "    sample = np.sort(sample)\n",
    "    return sample\n",
    "\n",
    "# Function that:\n",
    "# - sample 80% of the data and select it as training set\n",
    "# - use training data to do cross validation (k=10)\n",
    "# - once it has the best parameters, train the model on the training set\n",
    "# - test results using test set and returns a percentage of right predictions\n",
    "def applyRandomForest():\n",
    "    # Pick 80% of data as training set\n",
    "    pickAsTraining = defineTraining(len(df), 0.8)\n",
    "    \n",
    "    # Add a column to our dataframe, default value will be 0\n",
    "    df['is_train'] = 0\n",
    "    \n",
    "    # For each element selected as training, set its value in 'is_train' to 1\n",
    "    for el in pickAsTraining :\n",
    "        df.loc[el,'is_train'] = 1\n",
    "    \n",
    "    # Split our dataset in test and train, depending on the value in 'is_train' column\n",
    "    test, train = df[df['is_train']==0], df[df['is_train']==1]\n",
    "    # Select the features that will be used for training (in order to not use the labels for training)\n",
    "    features = df.columns[:22] \n",
    "    \n",
    "    # Initialize our classifier\n",
    "    clf = MyRandomForest()\n",
    "    \n",
    "    # Cross validation - BEGIN \n",
    "    # {\n",
    "    \n",
    "    # Create arrays in which I'll save the possible values of parameters to be used in RF  \n",
    "    n_estimators = [800, 1000, 1200]\n",
    "    max_depth = [60, 100, 140]\n",
    "    max_depth.append(None)\n",
    "    max_features = [2, 5, 11]\n",
    "    min_samples_leaf = [7, 10, 13]\n",
    "    min_samples_split = [2, 4, 6]\n",
    "    \n",
    "    # Create the 'random grid' containing the (selected) possible parameters for RF\n",
    "    random_grid = {\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_depth': max_depth,\n",
    "        'max_features': max_features,\n",
    "        'min_samples_leaf': min_samples_leaf,\n",
    "        'min_samples_split': min_samples_split\n",
    "    }\n",
    "    \n",
    "    # Define an object representing the CV method, already implemented, called on our classifier\n",
    "    grid_search = GridSearchCV(estimator = clf, param_grid = random_grid, cv = 10, n_jobs = -1, verbose = 1)\n",
    "    \n",
    "    # Call the CV model\n",
    "    grid_search.fit(train[features], train['presence'])\n",
    "        \n",
    "    # }    \n",
    "    # Cross Validation - END\n",
    "    \n",
    "    # Redefine our classifier with the found parameters\n",
    "    clf = MyRandomForest(\n",
    "        random_state = 0,\n",
    "        n_estimators = grid_search.best_params_[\"n_estimators\"],\n",
    "        max_depth = grid_search.best_params_[\"max_depth\"],\n",
    "        max_features = grid_search.best_params_[\"max_features\"],\n",
    "        min_samples_leaf = grid_search.best_params_[\"min_samples_leaf\"],\n",
    "        min_samples_split = grid_search.best_params_[\"min_samples_split\"]\n",
    "    )\n",
    "    \n",
    "    clf.fit(train[features], train['presence'])\n",
    "    predictions = clf.predict(test[features])\n",
    "    matching = [predictions == test['presence'].values]\n",
    "    totalMatches = np.sum(matching) / len(predictions)\n",
    "    \n",
    "    return totalMatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Suppress a warning\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Call K times the function \"applyRandomForest()\"\n",
    "K = 20\n",
    "results = np.zeros(K)\n",
    "\n",
    "for i in range(0,K):\n",
    "    #if (i%(int)(K/10) == 0):\n",
    "    #    print ((int)(i/K*100), \"%\")\n",
    "    results[i] = applyRandomForest()\n",
    "\n",
    "#print(results)    \n",
    "print(np.average(results))\n",
    "print(np.std(results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
